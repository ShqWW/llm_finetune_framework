model_path = 'remote_scripts/Baichuan-7B'
output_dir =  "saved_files/Baichuan_7B_QLoRA_t32"
data_path = 'data/chinese_thesis'
per_device_train_batch_size =  8
gradient_accumulation_steps =  1
learning_rate = 1e-3
num_train_epochs =  5
lr_scheduler_type = "linear"
warmup_ratio =  0.1
logging_steps = 10
save_strategy = "steps"
save_steps = 500
optim = "adamw_torch"
fp16 = False
remove_unused_columns = False
ddp_find_unused_parameters = False
seed = 42
lora_rank = 4
lora_alpha = 32
lora_dropout = 0.05
compute_dtype='fp32'  # you can choose ['fp32', 'fp16', 'bf16']
max_len = 3000
eos_token_id=2
pad_token_id=0
